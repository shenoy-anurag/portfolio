<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="theme-color" content="#663399"/><meta data-react-helmet="true" name="twitter:description" content="My personal website"/><meta data-react-helmet="true" name="twitter:title" content="Text Classification on Newsgroup dataset | Anurag Shenoy&#x27;s Blog"/><meta data-react-helmet="true" name="twitter:creator" content="Anurag Shenoy"/><meta data-react-helmet="true" name="twitter:card" content="summary"/><meta data-react-helmet="true" property="og:type" content="website"/><meta data-react-helmet="true" property="og:description" content="My personal website"/><meta data-react-helmet="true" property="og:title" content="Text Classification on Newsgroup dataset | Anurag Shenoy&#x27;s Blog"/><meta data-react-helmet="true" name="description" content="My personal website"/><meta name="generator" content="Gatsby 4.12.1"/><style data-href="/styles.00bbf464be3b3b2f46f6.css" data-identity="gatsby-global-css">@import url(https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Open+Sans&display=swap);@import url(https://fonts.googleapis.com/css2?family=Anonymous+Pro&display=swap);@import url(https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap);code[class*=language-],pre[class*=language-]{word-wrap:normal;background:none;color:#ccc;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:1em;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;line-height:1.5;-o-tab-size:4;tab-size:4;text-align:left;white-space:pre;word-break:normal;word-spacing:normal}pre[class*=language-]{margin:.5em 0;overflow:auto;padding:1em}:not(pre)>code[class*=language-],pre[class*=language-]{background:#2d2d2d}:not(pre)>code[class*=language-]{padding:.1em}.token.block-comment,.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#999}.token.punctuation{color:#ccc}.token.attr-name,.token.deleted,.token.namespace,.token.tag{color:#e2777a}.token.function-name{color:#6196cc}.token.boolean,.token.function,.token.number{color:#f08d49}.token.class-name,.token.constant,.token.property,.token.symbol{color:#f8c555}.token.atrule,.token.builtin,.token.important,.token.keyword,.token.selector{color:#cc99cd}.token.attr-value,.token.char,.token.regex,.token.string,.token.variable{color:#7ec699}.token.entity,.token.operator,.token.url{color:#67cdcc}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}.token.inserted{color:green}.card{align-items:center;background-position:50%;background-repeat:no-repeat;background-size:cover;display:flex;height:250px;justify-content:center;overflow:hidden!important;width:100%}.card .content{cursor:pointer;font-family:Open Sans;opacity:0;padding:10px;text-align:center;-webkit-transform:translateY(20px);transform:translateY(20px);transition:all .5s ease-in-out}.card .content h1{color:#fff;font-size:28px}.card .content p{color:#fff;font-size:16px;margin-bottom:20px}.card .content .btn{background-color:#000;color:#fff;padding:10px;text-decoration:none}.card .content:hover{opacity:1;-webkit-transform:translate(0);transform:translate(0)}.navbar-wrapper{align-items:center;display:flex;justify-content:space-between;margin-top:10vh;width:80vw}.navbar-wrapper .name{font-weight:600}.navbar-wrapper .home{background-color:transparent;border:0;cursor:pointer;font-family:Open Sans;outline:none;padding:10px;text-decoration:none}.navbar-wrapper .active{color:#000}.navbar-wrapper .links-wrapper button{background-color:transparent;border:0;cursor:pointer;font-family:Open Sans;font-size:12px;opacity:.6;outline:none;padding:10px;text-decoration:none;transition:all .2s ease-in-out}.navbar-wrapper .links-wrapper button:hover{opacity:1}.navbar-wrapper .links-wrapper .link{background-color:transparent;border:0;cursor:pointer;font-family:Open Sans;font-size:12px;opacity:.6;outline:none;padding:10px;text-decoration:none;transition:all .2s ease-in-out}.navbar-wrapper .links-wrapper .link:hover{opacity:1}.navbar-wrapper .links-wrapper .active{color:#000}.header-wrapper{align-items:center;display:flex;flex-direction:column;margin-top:10vh;width:80vw}.header-wrapper h2{font-size:1rem;text-align:center}@media (min-width:320px) and (max-width:424px){.header-wrapper h2{font-size:14px}}@media (min-width:425px) and (max-width:767px){.header-wrapper h2{font-size:16px}}@media (min-width:768px) and (max-width:1023px){.header-wrapper h2{font-size:16px}}.header-wrapper .heading-wrapper h1{font-size:4rem;text-align:center}@media (min-width:320px) and (max-width:424px){.header-wrapper .heading-wrapper h1{font-size:1.5rem}}@media (min-width:425px) and (max-width:767px){.header-wrapper .heading-wrapper h1{font-size:1.5rem}}@media (min-width:768px) and (max-width:1023px){.header-wrapper .heading-wrapper h1{font-size:2.5rem}}.header-wrapper p{overflow:hidden;text-align:center;width:50%}@media (min-width:320px) and (max-width:424px){.header-wrapper p{font-size:12px;width:100%}}@media (min-width:425px) and (max-width:767px){.header-wrapper p{font-size:12px;width:100%}}@media (min-width:768px) and (max-width:1023px){.header-wrapper p{font-size:14px;width:100%}}.work-wrapper{align-items:center;display:flex;flex-direction:column;margin-top:10vh;width:80vw}.work-wrapper h1{font-size:3rem;line-height:20px}.work-wrapper .grid{grid-gap:10px;display:grid;grid-template-columns:repeat(3,1fr);margin-top:20px}@media (min-width:320px) and (max-width:424px){.work-wrapper .grid{grid-template-columns:1fr}}@media (min-width:425px) and (max-width:767px){.work-wrapper .grid{grid-template-columns:1fr}}@media (min-width:768px) and (max-width:1023px){.work-wrapper .grid{grid-template-columns:1fr 1fr}}.about-section{display:flex;justify-content:space-between;margin-top:10vh;width:80vw}@media (min-width:320px) and (max-width:424px){.about-section{flex-direction:column-reverse}}@media (min-width:425px) and (max-width:767px){.about-section{flex-direction:column-reverse}}@media (min-width:768px) and (max-width:1023px){.about-section{align-items:center;flex-direction:column-reverse}}.about-section .content{width:50%}@media (min-width:320px) and (max-width:424px){.about-section .content{width:100%}}@media (min-width:425px) and (max-width:767px){.about-section .content{width:100%}}@media (min-width:768px) and (max-width:1023px){.about-section .content{width:100%}}.about-section .content h1{font-size:3rem;line-height:20px}@media (min-width:768px) and (max-width:1023px){.about-section .content h1,.about-section .content p{text-align:center}}.about-section .image-wrapper{align-items:top;display:flex;justify-content:center;overflow:hidden;width:50%}@media (min-width:320px) and (max-width:424px){.about-section .image-wrapper{width:100%}}@media (min-width:425px) and (max-width:767px){.about-section .image-wrapper{width:100%}}@media (min-width:768px) and (max-width:1023px){.about-section .image-wrapper{width:100%}}.skills-container{align-items:center;display:flex;flex-direction:column;margin-top:10vh;width:80vw}.skills-container h1{font-size:3rem;line-height:20px}.skills-container .skills-grid{grid-gap:20px;display:grid;grid-template-columns:repeat(3,1fr);margin-top:50px}@media (min-width:320px) and (max-width:424px){.skills-container .skills-grid{grid-template-columns:1fr}}@media (min-width:425px) and (max-width:767px){.skills-container .skills-grid{grid-template-columns:1fr}}@media (min-width:768px) and (max-width:1023px){.skills-container .skills-grid{grid-template-columns:1fr 1fr}}.skills-container .skills-grid .skill{align-items:center;display:flex;flex-direction:column;text-align:center}.skills-container .skills-grid .skill img{height:3rem}.skills-container .skills-grid .skill p{font-size:14px}.skills-container .skills-grid .skill .beginner{color:#3d903d}.skills-container .skills-grid .skill .intermediate{color:#ef6731}.skills-container .skills-grid .skill .advanced{color:darkred}.skills-container .skills-grid .skill .expert{color:purple}.promotion-container{align-items:center;display:flex;flex-direction:column;margin-top:10vh;text-align:center;width:80vw}.promotion-container h1{font-size:3rem;line-height:20px}.footer-container{align-items:center;display:flex;flex-direction:column;margin-bottom:10vh;margin-top:10vh;width:80vw}.footer-container h1{font-size:3rem;line-height:20px}.footer-container h2{font-size:1.5rem;overflow:hidden}@media (min-width:320px) and (max-width:424px){.footer-container h2{display:none}}@media (min-width:425px) and (max-width:767px){.footer-container h2{font-size:14px}}@media (min-width:768px) and (max-width:1023px){.footer-container h2{font-size:1.2rem}}.footer-container .email-link{color:#000;font-size:1.5rem}.footer-container .social-icons{margin-top:20px}.footer-container .social-icons img{height:2rem;margin:10px}.footer-container span{margin-top:10px}.footer-container icon{color:red}html{--bg:#fff;--bg-secondary:#f1f1f1;--headings:#000;--text:#333;--links:blue;--highlight:#ffecb2;--code-text:#9d174d;--share-text:#999}.blog-wrapper{align-items:center;display:flex;flex-direction:column;margin-top:10vh;width:80vw}@media (min-width:320px) and (max-width:424px){.blog-wrapper{width:80%}}@media (min-width:425px) and (max-width:767px){.blog-wrapper{width:80%}}.blog-wrapper h1{font-size:3rem;line-height:20px}.blog-wrapper .btn{background-color:#000;color:#fff;margin-top:5vh;padding:10px 15px;text-align:center;text-decoration:none}.blog-wrapper .heavy{text-transform:uppercase}.blog-post-container{align-items:center;display:flex;flex-direction:column;height:auto;overflow:hidden;width:100vw}@media (min-width:320px) and (max-width:424px){.blog-post-container{width:80%}}@media (min-width:425px) and (max-width:767px){.blog-post-container{width:80%}}.blog-post-container .blog-post{align-items:center;display:flex;flex-direction:column;height:100%;justify-content:center;width:80%}.blog-post-container .blog-post h1{align-items:center}.blog-post-container .blog-post .blog-post-content{display:flex;flex-direction:column;font-family:Inter,-apple-system,avenir next,avenir,roboto,noto,ubuntu,helvetica neue,helvetica,sans-serif;font-size:1rem;line-height:2;margin-top:3vh;max-width:900px;width:80vw}.blog-post-container .blog-post .blog-post-content h1{font-size:3rem;line-height:20px}.post-date{color:#2a2a2a;font-size:.8em;font-weight:700;text-transform:uppercase}.token{font-family:Bitstream Vera Sans Mono,Consolas,Courier,monospace;font-size:.9rem}:not(pre)>code[class*=language-]{background-color:var(--bg-secondary);border-radius:.3em;color:var(--code-text);font-weight:700;padding:.2rem;white-space:normal}.gatsby-highlight-code-line{background-color:#1a1f35;border-left:.25em solid #f99;color:#f2f0ec;display:block;margin-left:-1em;margin-right:-1em;padding-left:.75em;padding-right:1em}.gatsby-highlight{background-color:#1a1f35;border-radius:.3em;color:#f2f0ec;margin:.5em 0;overflow:auto;padding:1em}.gatsby-highlight pre[class*=language-]{background-color:transparent;float:left;margin:0;min-width:100%;overflow:initial;padding:0}.gatsby-highlight pre[class*=language-].line-numbers{padding-left:2.8em}.blog-card{align-items:left;display:flex;flex-direction:column;margin-top:5vh;width:60vw}@media (min-width:320px) and (max-width:424px){.blog-card{width:90%}}@media (min-width:425px) and (max-width:767px){.blog-card{width:90%}}.blog-card .content{cursor:pointer;opacity:1;padding:10px;text-align:left}.blog-card .content h1{color:#0071c2;font-size:24px;line-height:24px}.blog-card .content p{color:#000;font-size:16px;margin-bottom:20px}.blog-card .content .date{color:#2a2a2a;font-size:.8em;font-weight:700;text-transform:uppercase}*{font-family:Inter,-apple-system,avenir next,avenir,roboto,noto,ubuntu,helvetica neue,helvetica,sans-serif}body{margin:0;overflow-x:hidden;overflow-y:scroll;padding:0}.section{height:auto;overflow:hidden;width:100vw}.container{align-items:center;display:flex;flex-direction:column;height:100%;justify-content:center;width:100%}.primary-btn{background-color:#000;border:0;color:#fff;cursor:pointer;font-size:16px;margin:10px;padding:10px 15px;text-decoration:none;text-transform:uppercase;transition:all .2s ease-in-out}.primary-btn:hover{-webkit-transform:scale(1.1);transform:scale(1.1)}</style><title data-react-helmet="true">Text Classification on Newsgroup dataset | Anurag Shenoy&#x27;s Blog | Portfolio</title><link rel="icon" href="/favicon-32x32.png?v=4a9773549091c227cd2eb82ccd9c5e3a" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=4a9773549091c227cd2eb82ccd9c5e3a"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=4a9773549091c227cd2eb82ccd9c5e3a"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=4a9773549091c227cd2eb82ccd9c5e3a"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=4a9773549091c227cd2eb82ccd9c5e3a"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=4a9773549091c227cd2eb82ccd9c5e3a"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=4a9773549091c227cd2eb82ccd9c5e3a"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=4a9773549091c227cd2eb82ccd9c5e3a"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=4a9773549091c227cd2eb82ccd9c5e3a"/><link as="script" rel="preload" href="/webpack-runtime-b5a1197b4dce08032c07.js"/><link as="script" rel="preload" href="/framework-0841a708e42dbed148bb.js"/><link as="script" rel="preload" href="/app-4025e550f650b75cd59a.js"/><link as="script" rel="preload" href="/commons-9d97cb314d58da02d715.js"/><link as="script" rel="preload" href="/component---src-pages-markdown-remark-frontmatter-slug-js-069fbe32344d374724d4.js"/><link as="fetch" rel="preload" href="/page-data/blog/text-classification-on-newsgroup-data/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/63159454.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="section"><div class="container"><div class="navbar-wrapper"><div role="button" class="name" tabindex="0"><a class="home active" href="/">Portfolio.</a></div><div class="links-wrapper"><a class="link active" href="/work">Work</a><a class="link active" href="/blog">Blog</a><a class="link active" href="/about">About</a><button>Contact</button></div></div></div></div><main><div class="blog-post-container"><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script><div class="blog-post"><h1>Text Classification on Newsgroup dataset</h1><p class="post-date"><span>February 01, 2022</span></p><div class="blog-post-content"><h2>Introduction</h2>
<p>There are various algorithms we can use to classify text. In Machine Learning, we often don't use any context or meaning representations to classify text. A document is often only represented as a bag of words. While this approach might seem too simplistic and not taking complex word interactions into account (you could say it's Naïve), it is usually good enough to classify a document as belonging to a particular class.</p>
<p>In this post, we'll be exploring 4 methods used to classify text using the bag-of-words approach, we'll see the code and math required, and then I'll leave you with improvements we could make to these techniques in order to improve accuracy.</p>
<p>The four algorithms we'll be using are:</p>
<ol>
<li>Bernoulli Naïve Bayes</li>
<li>Multinomial Naïve Bayes</li>
<li>Linear Discriminant Analysis (with TF-IDF pre-processing)</li>
<li>Least Squares (with TF-IDF)</li>
</ol>
<h2>Naïve Bayes <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes">1</a></h2>
<p>The equation for Naive Bayes classification rule is,</p>
<p>$$\begin{align*}\begin{aligned}P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)\\Downarrow\\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y)\end{aligned}\end{align*}$$</p>
<br>
The probability $$P(x_i|y)$$ represents the probability of feature $$i$$ in training data $$x$$ given that we know the document belongs to a class $$y$$.
<br>
We multiply the probabilities of each of the features appearing given that they belong to class $$y$$ i.e. $$\prod_{i=1}^n P(x_i|y)$$ and to obtain the overall probability, we multiply the result with the probability that a document belongs to the class $$y$$ in the set of all documents. We calculate these probabilities for every class and then compare them. 
<p><strong>The largest probability is the predicted class.</strong></p>
<p><strong>Let's understand this through an example:</strong></p>
<p>Say we have the following words: <code class="language-text">Gold</code>, <code class="language-text">Bank</code>, <code class="language-text">Dear</code>, <code class="language-text">Hangout</code> and we wanted to find out whether the email was spam or normal, then we can use Bayes to figure that out, effectively classifying it as spam (S) or not spam (N).</p>
<p>Let's say we calculated the following probabilities from word histograms:</p>
<p>$$ P(S) = 0.33 $$</p>
<p>$$ P(Gold|S) = 0.52 $$</p>
<p>$$ P(Bank|S) = 0.35 $$</p>
<p>$$ P(Dear|S) = 0.07 $$</p>
<p>$$ P(Hangout|S) = 0.00 $$</p>
<p>$$ P(N) = 0.67 $$</p>
<p>$$ P(Gold|N) = 0.02 $$</p>
<p>$$ P(Bank|N) = 0.07 $$</p>
<p>$$ P(Dear|N) = 0.68 $$</p>
<p>$$ P(Hangout|N) = 0.23 $$</p>
<p>Say our message is "Dear Bank", the probability that it belongs to class N is:
$$ P(N) * (P(Dear|N) * P(Bank|N)) = 0.0319 $$</p>
<p>And, the probability that it belongs to class S is:
$$ P(S) * (P(Dear|S) * P(Bank|S)) = 0.008 $$</p>
<p>As 0.0319 > 0.008, argmax function will tell us that this email belongs to class N, i.e. it is a normal email.</p>
<p>Say now our message is "Hangout Bank Gold Gold", the probability that it belongs to class N is:
$$ P(N) * (P(Hangout|N) * P(Bank|N) * P(Gold|N)^2) = 4.3148 \text{x} 10^{-6} $$</p>
<p>And, the probability that it belongs to class S is:
$$ P(S) * (P(Hangout|S) * P(Bank|S) * P(Gold|S)^2) = 0.00 $$</p>
<br>
Even though we have a good idea that this should be classified as spam, the entire probability becomes zero as $$ P(Hangout|S)) = 0.0 $$
<p>We have two issues here:</p>
<ul>
<li>Zero probabilities</li>
<li>Underflow (values so small, they are basically zero)</li>
</ul>
<p>To address the issue of <strong>Zero Probabilities</strong>, we add some count (usually 1) to each word's frequency, thus obtaining non-zero probabilities for each feature. This is usually denoted by $$\alpha$$.</p>
<br>
Also, as you can see, $$ P(N) * (P(Hangout|N) * P(Bank|N) * P(Gold|N)^2) $$ results in a very tiny number. If we had 100 features/words, this number might become too small for computer systems to store as there may not be enough precision in even `float64` data type.
<p>To address the issue of <strong>Underflow</strong>, we usually take <code class="language-text">log</code> of all the probabilities and compare the log products. Since we are only comparing the relative magnitudes, any transformation that preserves proportionality while keeping the numbers large enough to store in <code class="language-text">float64</code> will work, and <code class="language-text">log</code> is perfect for this.</p>
<h3>Bernoulli Naïve Bayes <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#bernoulli-naive-bayes">2</a></h3>
<p>The decision rule for Bernoulli Naive Bayes is:</p>
<p>$$P(x_i \mid y) = P(i \mid y) x_i + (1 - P(i \mid y)) (1 - x_i)$$</p>
<h3>Multinomial Naïve Bayes <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes">3</a></h3>
<p>If you want to skip to the code, please check <a href="#data-pre-processing">data pre-processing</a> first and then the code for MultinomialNB <a href="#multinomial-naive-bayes-classifier">here</a>.</p>
<p>Now that we have that out of the way, let's take a look at the math.</p>
<p>In Multinomial Naive Bayes, we apply Naive Bayes to data that is multinomially distributed (where the data is represented by counts of different features). In text classification, this means our data uses word counts.</p>
<p>It simply tells us that the each of the $$P(x_i \mid y)$$ follows a multinomial distribution.</p>
<p>The distribution is parametrized by vectors $$ \theta_y = (\theta_{y1},\ldots,\theta_{yn}) $$
for each class $$y$$, where $$n$$ is the number of features (in text classification, the size of the vocabulary) and $$\theta_{yi}$$
is the probability $$P(x_i \mid y)$$
of feature $$i$$ appearing in a sample belonging to class $$y$$.</p>
<p>We can estimate the parameters $$\theta_y$$:</p>
<p>$$\hat{\theta}<em>{yi} = \frac{ N</em>{yi} + \alpha}{N_y + \alpha n}$$</p>
<p>Where $$N_{yi}$$ is the number of time a feature $$i$$ appears in class $$y$$ in the training data and $$N_y$$ is the total count of all features for class $$y$$.</p>
<p>Sci-kit learn also performs smoothing by adding 1 to every feature and prevents zero probabilities in calculations (denoted by $$\alpha$$ which we covered in Naive Bayes section).</p>
<h3>TF-IDF, LDA and Least Squares</h3>
<p>These two topics involve a lot more math and I feel they deserve a separate post. Until then, please see the <a href="#references-and-further-reading">last section</a> for reading materials.</p>
<p>If you want to skip to the code, LDA is <a href="#lda-with-tf-idf">here</a> and Least Squares is covered <a href="#least-squares">here</a>.</p>
<h2>Dataset</h2>
<p>The data we'll be using is the Newsgroup dataset which can be found here: <a href="http://qwone.com/~jason/20Newsgroups/">http://qwone.com/~jason/20Newsgroups/</a></p>
<p>We'll be using the Matlab/Octave processed version of the data.</p>
<h2>The code and explanation</h2>
<p>We'll start by importing important libraries</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> tarfile
<span class="token keyword">import</span> requests
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt <span class="token comment"># for data visualization purposes</span>
<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns <span class="token comment"># for statistical data visualization</span>
<span class="token keyword">import</span> sklearn <span class="token keyword">as</span> sk
<span class="token operator">%</span>matplotlib inline

<span class="token comment"># Use 3 decimal places in output display</span>
pd<span class="token punctuation">.</span>set_option<span class="token punctuation">(</span><span class="token string">"display.precision"</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token comment"># Don't wrap repr(DataFrame) across additional lines</span>
pd<span class="token punctuation">.</span>set_option<span class="token punctuation">(</span><span class="token string">"display.expand_frame_repr"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token comment"># Set max rows displayed in output to 25</span>
pd<span class="token punctuation">.</span>set_option<span class="token punctuation">(</span><span class="token string">"display.max_rows"</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">)</span></code></pre></div>
<h4>Data Pre-processing</h4>
<p>Let's download the newgroup data and extract it.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">NEWSGROUP_DATASET_URL <span class="token operator">=</span> <span class="token string">"http://qwone.com/~jason/20Newsgroups/20news-bydate-matlab.tgz"</span>
gzip_file_name <span class="token operator">=</span> NEWSGROUP_DATASET_URL<span class="token punctuation">.</span>rsplit<span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
NEWSGROUP_FOLDER <span class="token operator">=</span> <span class="token string">"20news-bydate"</span>
NEWSGROUP_GZIP_PATH <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">,</span> gzip_file_name<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>NEWSGROUP_GZIP_PATH<span class="token punctuation">)</span>

<span class="token comment"># downloading the file</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>NEWSGROUP_GZIP_PATH<span class="token punctuation">)</span><span class="token punctuation">:</span>
    req <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>NEWSGROUP_DATASET_URL<span class="token punctuation">,</span> allow_redirects<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token comment"># Writing the file to the local file system</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>NEWSGROUP_GZIP_PATH<span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> output_file<span class="token punctuation">:</span>
        output_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span>req<span class="token punctuation">.</span>content<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Downloaded zip file"</span><span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Zip file already present"</span><span class="token punctuation">)</span>

<span class="token comment"># open file</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>NEWSGROUP_FOLDER<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> tarfile<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>NEWSGROUP_GZIP_PATH<span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        <span class="token comment"># extracting file</span>
        f<span class="token punctuation">.</span>extractall<span class="token punctuation">(</span><span class="token string">'.'</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"De-compressed Gzip file"</span><span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Un-compressed folder already present"</span><span class="token punctuation">)</span></code></pre></div>
<p>Now, we can read and process the data.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">DATA_PATH <span class="token operator">=</span> <span class="token string">"./20news-bydate/matlab/"</span>

TRAIN_DATA_FILE <span class="token operator">=</span> <span class="token string">"train.data"</span>
TRAIN_LABEL_FILE <span class="token operator">=</span> <span class="token string">"train.label"</span>
TRAIN_MAP_FILE <span class="token operator">=</span> <span class="token string">"train.map"</span>

TEST_DATA_FILE <span class="token operator">=</span> <span class="token string">"test.data"</span>
TEST_LABEL_FILE <span class="token operator">=</span> <span class="token string">"test.label"</span>
TEST_MAP_FILE <span class="token operator">=</span> <span class="token string">"test.map"</span>

train_data_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>DATA_PATH<span class="token punctuation">,</span> TRAIN_DATA_FILE<span class="token punctuation">)</span>
test_data_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>DATA_PATH<span class="token punctuation">,</span> TEST_DATA_FILE<span class="token punctuation">)</span>
train_label_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>DATA_PATH<span class="token punctuation">,</span> TRAIN_LABEL_FILE<span class="token punctuation">)</span>
test_label_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>DATA_PATH<span class="token punctuation">,</span> TEST_LABEL_FILE<span class="token punctuation">)</span>
train_map_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>DATA_PATH<span class="token punctuation">,</span> TRAIN_MAP_FILE<span class="token punctuation">)</span>
test_map_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>DATA_PATH<span class="token punctuation">,</span> TEST_MAP_FILE<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">read_data_file</span><span class="token punctuation">(</span>file_path<span class="token punctuation">)</span><span class="token punctuation">:</span>
    lines <span class="token operator">=</span> <span class="token boolean">None</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>file_path<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        lines <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
    lines <span class="token operator">=</span> lines<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span>
    lines <span class="token operator">=</span> <span class="token punctuation">[</span>line <span class="token keyword">for</span> line <span class="token keyword">in</span> lines <span class="token keyword">if</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> lines

<span class="token keyword">def</span> <span class="token function">load_data</span><span class="token punctuation">(</span>file_path<span class="token punctuation">)</span><span class="token punctuation">:</span>
    df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>file_path<span class="token punctuation">,</span> names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"doc_id"</span><span class="token punctuation">,</span> <span class="token string">"word_id"</span><span class="token punctuation">,</span> <span class="token string">"count"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">' '</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> df

<span class="token keyword">def</span> <span class="token function">load_labels</span><span class="token punctuation">(</span>file_path<span class="token punctuation">)</span><span class="token punctuation">:</span>
    df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>file_path<span class="token punctuation">,</span> names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"label_id"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sep<span class="token operator">=</span><span class="token string">' '</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> df

<span class="token keyword">def</span> <span class="token function">load_map</span><span class="token punctuation">(</span>map_file_path<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_map_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    train_map <span class="token operator">=</span> read_data_file<span class="token punctuation">(</span>map_file_path<span class="token punctuation">)</span>
    <span class="token keyword">for</span> line <span class="token keyword">in</span> train_map<span class="token punctuation">:</span>
        <span class="token keyword">if</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            k<span class="token punctuation">,</span> v <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
            train_map_dict<span class="token punctuation">[</span>k<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span>
    <span class="token keyword">return</span> train_map_dict

train_df <span class="token operator">=</span> load_data<span class="token punctuation">(</span>train_data_path<span class="token punctuation">)</span>
train_df<span class="token punctuation">.</span>dropna<span class="token punctuation">(</span><span class="token punctuation">)</span>
test_df <span class="token operator">=</span> load_data<span class="token punctuation">(</span>test_data_path<span class="token punctuation">)</span>
test_df<span class="token punctuation">.</span>dropna<span class="token punctuation">(</span><span class="token punctuation">)</span>

train_labels <span class="token operator">=</span> load_labels<span class="token punctuation">(</span>train_label_path<span class="token punctuation">)</span>
train_labels<span class="token punctuation">.</span>dropna<span class="token punctuation">(</span><span class="token punctuation">)</span>
test_labels <span class="token operator">=</span> load_labels<span class="token punctuation">(</span>test_label_path<span class="token punctuation">)</span>
test_labels<span class="token punctuation">.</span>dropna<span class="token punctuation">(</span><span class="token punctuation">)</span>

train_map <span class="token operator">=</span> load_map<span class="token punctuation">(</span>train_map_path<span class="token punctuation">)</span>
test_map <span class="token operator">=</span> load_map<span class="token punctuation">(</span>test_map_path<span class="token punctuation">)</span>
labels_to_label_ids <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token operator">**</span>train_map<span class="token punctuation">,</span> <span class="token operator">**</span>test_map<span class="token punctuation">}</span>
label_ids_to_labels <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span>v<span class="token punctuation">,</span> k<span class="token punctuation">)</span> <span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> labels_to_label_ids<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">del</span> train_map
<span class="token keyword">del</span> test_map</code></pre></div>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python">train_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>train_df<span class="token punctuation">,</span> train_labels<span class="token punctuation">,</span> left_on<span class="token operator">=</span><span class="token string">"doc_id"</span><span class="token punctuation">,</span> right_index<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
test_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>test_df<span class="token punctuation">,</span> test_labels<span class="token punctuation">,</span> left_on<span class="token operator">=</span><span class="token string">"doc_id"</span><span class="token punctuation">,</span> right_index<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
train_df</code></pre></div>
<p>So our training and test inputs should look like this:</p>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">	doc_id	word_id	count	label_id
0	1	1	4	1
1	1	2	2	1
2	1	3	10	1
3	1	4	4	1
4	1	5	2	1
...	...	...	...	...
1467128	11268	25975	2	20
1467129	11268	27356	1	20
1467130	11268	53958	1	20
1467131	11268	53974	1	20
1467132	11268	53975	1	20
1467133 rows × 4 columns</code></pre></div>
<p>To simplify computation, lets use a smaller vocabulary instead of the complete vocabulary.</p>
<p>We'll keep only those words which have appeared more than 1000 times overall across the set of all the documents.</p>
<p>This could have the effect of reducing the accuracy as we'll be keeping words such as articles ("a", "an" and "the") and other regularly used fluff words which don't really mean much by themselves, and can often be removed without changing the meaning of the sentence. These words are called <strong><em>stopwords</em></strong>.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Pruning data based on word count:</span>
<span class="token comment"># We will keep only those words which have appeared more than 1000 times in the training data.</span>
MIN_WORD_COUNT <span class="token operator">=</span> <span class="token number">1000</span>

grouped <span class="token operator">=</span> train_df<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span><span class="token string">"word_id"</span><span class="token punctuation">)</span>
grouped <span class="token operator">=</span> grouped<span class="token punctuation">.</span>agg<span class="token punctuation">(</span>total_word_count<span class="token operator">=</span>pd<span class="token punctuation">.</span>NamedAgg<span class="token punctuation">(</span>column<span class="token operator">=</span><span class="token string">"count"</span><span class="token punctuation">,</span> aggfunc<span class="token operator">=</span><span class="token string">"sum"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
df <span class="token operator">=</span> grouped<span class="token punctuation">.</span>reset_index<span class="token punctuation">(</span><span class="token punctuation">)</span>
above_1000 <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">[</span><span class="token string">"total_word_count"</span><span class="token punctuation">]</span> <span class="token operator">></span> MIN_WORD_COUNT<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>above_1000<span class="token punctuation">[</span><span class="token string">"total_word_count"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>describe<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># Words with more than 1000 total appearances</span>
final_word_list <span class="token operator">=</span> above_1000<span class="token punctuation">[</span><span class="token string">"word_id"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
N_WORDS <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>final_word_list<span class="token punctuation">)</span>
words_to_idx <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">[</span>final_word_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>final_word_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># Prune train df</span>
df <span class="token operator">=</span> train_df<span class="token punctuation">[</span><span class="token string">"word_id"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>isin<span class="token punctuation">(</span>final_word_list<span class="token punctuation">)</span>
train_df <span class="token operator">=</span> train_df<span class="token punctuation">[</span>df<span class="token punctuation">]</span>
<span class="token comment"># Prune test df</span>
df <span class="token operator">=</span> test_df<span class="token punctuation">[</span><span class="token string">"word_id"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>isin<span class="token punctuation">(</span>final_word_list<span class="token punctuation">)</span>
test_df <span class="token operator">=</span> test_df<span class="token punctuation">[</span>df<span class="token punctuation">]</span>

<span class="token comment"># Getting the list of all unique document ids</span>
train_doc_ids <span class="token operator">=</span> train_df<span class="token punctuation">[</span><span class="token string">"doc_id"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
test_doc_ids <span class="token operator">=</span> test_df<span class="token punctuation">[</span><span class="token string">"doc_id"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
final_doc_id_list <span class="token operator">=</span> train_doc_ids <span class="token operator">+</span> test_doc_ids
final_doc_id_list<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">del</span> train_doc_ids
<span class="token keyword">del</span> test_doc_ids
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>final_doc_id_list<span class="token punctuation">)</span><span class="token punctuation">,</span> final_doc_id_list<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">18760 [1, 1, 2, 2, 3]</code></pre></div>
<p>Writing the functions for creating Bernoulli and Multinomial features:</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">encode_doc_bernoulli</span><span class="token punctuation">(</span>n_words<span class="token punctuation">,</span> doc_group<span class="token punctuation">)</span><span class="token punctuation">:</span>
    word_counts_enc_list <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_words<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
    <span class="token keyword">for</span> row <span class="token keyword">in</span> doc_group<span class="token punctuation">.</span>itertuples<span class="token punctuation">(</span>index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        word_counts_enc_list<span class="token punctuation">[</span>words_to_idx<span class="token punctuation">[</span>row<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
    <span class="token keyword">return</span> word_counts_enc_list

<span class="token keyword">def</span> <span class="token function">encode_doc_multinomial</span><span class="token punctuation">(</span>n_words<span class="token punctuation">,</span> doc_group<span class="token punctuation">)</span><span class="token punctuation">:</span>
    word_counts_enc_list <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>n_words<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>
    <span class="token keyword">for</span> row <span class="token keyword">in</span> doc_group<span class="token punctuation">.</span>itertuples<span class="token punctuation">(</span>index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        word_counts_enc_list<span class="token punctuation">[</span>words_to_idx<span class="token punctuation">[</span>row<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> row<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> word_counts_enc_list
    
<span class="token keyword">def</span> <span class="token function">encode_documents</span><span class="token punctuation">(</span>df<span class="token punctuation">,</span> group_col<span class="token punctuation">,</span> n_words<span class="token punctuation">,</span> enc_type<span class="token operator">=</span><span class="token string">"Bernoulli"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    docs_word_count_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    doc_ids <span class="token operator">=</span> df<span class="token punctuation">[</span>group_col<span class="token punctuation">]</span><span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
    doc_ids<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span>
    grouped <span class="token operator">=</span> df<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span>group_col<span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> enc_type <span class="token operator">==</span> <span class="token string">"Bernoulli"</span><span class="token punctuation">:</span>
        enc_fn <span class="token operator">=</span> encode_doc_bernoulli
    <span class="token keyword">elif</span> enc_type <span class="token operator">==</span> <span class="token string">"Multinomial"</span><span class="token punctuation">:</span>
        enc_fn <span class="token operator">=</span> encode_doc_multinomial
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        enc_fn <span class="token operator">=</span> encode_doc_bernoulli
    
    <span class="token keyword">for</span> doc_id <span class="token keyword">in</span> doc_ids<span class="token punctuation">:</span>
        group <span class="token operator">=</span> grouped<span class="token punctuation">.</span>get_group<span class="token punctuation">(</span>doc_id<span class="token punctuation">)</span>
        word_counts_enc_list <span class="token operator">=</span> enc_fn<span class="token punctuation">(</span>n_words<span class="token punctuation">,</span> group<span class="token punctuation">)</span>
        docs_word_count_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word_counts_enc_list<span class="token punctuation">)</span>
    <span class="token keyword">return</span> docs_word_count_list

<span class="token keyword">def</span> <span class="token function">get_labels_for_doc_ids</span><span class="token punctuation">(</span>df<span class="token punctuation">)</span><span class="token punctuation">:</span>
    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    doc_ids <span class="token operator">=</span> df<span class="token punctuation">[</span><span class="token string">"doc_id"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
    grouped <span class="token operator">=</span> df<span class="token punctuation">.</span>groupby<span class="token punctuation">(</span><span class="token string">"doc_id"</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> doc_id <span class="token keyword">in</span> doc_ids<span class="token punctuation">:</span>
        group <span class="token operator">=</span> grouped<span class="token punctuation">.</span>get_group<span class="token punctuation">(</span>doc_id<span class="token punctuation">)</span>
        labels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>group<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> labels</code></pre></div>
<p>The <code class="language-text">encode_documents()</code> function will process the df and return an array of document vectors where each vector contains all of the words in our vocabulary. In the case when we want <strong>Bernoulli</strong> features, we will represent each word by a <code class="language-text">0</code> if not present in the document, or a <code class="language-text">1</code> if it is present.</p>
<h4>Let's start with Bernoulli Naïve Bayes:</h4>
<p>The <code class="language-text">BernoulliNB</code> class of <code class="language-text">sklearn</code> library will return a model which can train on the array of document vectors and can then be used to predict the class for a new document.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Imports for clasification</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>naive_bayes <span class="token keyword">import</span> BernoulliNB
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics

x_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>encode_documents<span class="token punctuation">(</span>train_df<span class="token punctuation">,</span> group_col<span class="token operator">=</span><span class="token string">"doc_id"</span><span class="token punctuation">,</span> n_words<span class="token operator">=</span>N_WORDS<span class="token punctuation">,</span> enc_type<span class="token operator">=</span><span class="token string">"Bernoulli"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x_test <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>encode_documents<span class="token punctuation">(</span>test_df<span class="token punctuation">,</span> group_col<span class="token operator">=</span><span class="token string">"doc_id"</span><span class="token punctuation">,</span> n_words<span class="token operator">=</span>N_WORDS<span class="token punctuation">,</span> enc_type<span class="token operator">=</span><span class="token string">"Bernoulli"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
y_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>get_labels_for_doc_ids<span class="token punctuation">(</span>train_df<span class="token punctuation">)</span><span class="token punctuation">)</span>
y_test <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>get_labels_for_doc_ids<span class="token punctuation">(</span>test_df<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># Classification using Bernoulli Naive Bayes</span>
bnb_model <span class="token operator">=</span> BernoulliNB<span class="token punctuation">(</span>binarize<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
bnb_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>

<span class="token comment"># Testing BernoulliNB model</span>
y_pred <span class="token operator">=</span> bnb_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
test_accuracy <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Bernoulli Naive Bayes has an accuracy of {:.2f}%"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>test_accuracy<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"against the test set."</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">Bernoulli Naive Bayes has an accuracy of 28.41% against the test set.</code></pre></div>
<h4>Multinomial Naive Bayes Classifier</h4>
<p>Next, we'll explore improving the accuracy by using Multinomial Naïve Bayes:</p>
<p>In Multinomial Naïve Bayes, instead of using a binary representation for a word in the vector for the document, we use the word frequency.</p>
<p>Intuitively, if a document has certain words in larger number like "football", "player", "Barcelona", "baseball", "athlete" etc, then it's more likely to fall under the <code class="language-text">sports</code> category/class as opposed to a document having the class <code class="language-text">politics</code> and mentioning the word "athlete" once.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># Imports for clasification</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>naive_bayes <span class="token keyword">import</span> MultinomialNB
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics

x_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>encode_documents<span class="token punctuation">(</span>train_df<span class="token punctuation">,</span> group_col<span class="token operator">=</span><span class="token string">"doc_id"</span><span class="token punctuation">,</span> n_words<span class="token operator">=</span>N_WORDS<span class="token punctuation">,</span> enc_type<span class="token operator">=</span><span class="token string">"Multinomial"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x_test <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>encode_documents<span class="token punctuation">(</span>test_df<span class="token punctuation">,</span> group_col<span class="token operator">=</span><span class="token string">"doc_id"</span><span class="token punctuation">,</span> n_words<span class="token operator">=</span>N_WORDS<span class="token punctuation">,</span> enc_type<span class="token operator">=</span><span class="token string">"Multinomial"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
y_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>get_labels_for_doc_ids<span class="token punctuation">(</span>train_df<span class="token punctuation">)</span><span class="token punctuation">)</span>
y_test <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>get_labels_for_doc_ids<span class="token punctuation">(</span>test_df<span class="token punctuation">)</span><span class="token punctuation">)</span>

mn_model <span class="token operator">=</span> MultinomialNB<span class="token punctuation">(</span><span class="token punctuation">)</span>
mn_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>

<span class="token comment"># Testing Multinomial Naive Bayes</span>
y_pred <span class="token operator">=</span> mn_model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>
test_accuracy <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Multinomial Naïve Bayes has an accuracy of {:.2f}%"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>test_accuracy<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"against the test set."</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">Multinomial Naïve Bayes has an accuracy of 39.02% against the test set.</code></pre></div>
<p>As we can see, using Multinomial encoding for document vector does improve the accuracy of classification.</p>
<h4>LDA with TF-IDF</h4>
<p>Let's see how it performs using TF-IDF and LDA.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfTransformer
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>discriminant_analysis <span class="token keyword">import</span> LinearDiscriminantAnalysis

x_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>encode_documents<span class="token punctuation">(</span>train_df<span class="token punctuation">,</span> group_col<span class="token operator">=</span><span class="token string">"doc_id"</span><span class="token punctuation">,</span> n_words<span class="token operator">=</span>N_WORDS<span class="token punctuation">,</span> enc_type<span class="token operator">=</span><span class="token string">"Multinomial"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x_test <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>encode_documents<span class="token punctuation">(</span>test_df<span class="token punctuation">,</span> group_col<span class="token operator">=</span><span class="token string">"doc_id"</span><span class="token punctuation">,</span> n_words<span class="token operator">=</span>N_WORDS<span class="token punctuation">,</span> enc_type<span class="token operator">=</span><span class="token string">"Multinomial"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
y_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>get_labels_for_doc_ids<span class="token punctuation">(</span>train_df<span class="token punctuation">)</span><span class="token punctuation">)</span>
y_test <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>get_labels_for_doc_ids<span class="token punctuation">(</span>test_df<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># TF-IDF processing:</span>
tfidf_transformer <span class="token operator">=</span> TfidfTransformer<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># train data processing</span>
x_train <span class="token operator">=</span> tfidf_transformer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># test data processing</span>
x_test <span class="token operator">=</span> tfidf_transformer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span>

clf <span class="token operator">=</span> LinearDiscriminantAnalysis<span class="token punctuation">(</span><span class="token punctuation">)</span>
clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>

y_pred <span class="token operator">=</span> clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>
test_accuracy <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>y_train<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"LDA model accuracy: {:.2f}%"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>test_accuracy<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">" on training data."</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">LDA model accuracy: 53.73%  on training data.</code></pre></div>
<p>The LDA model with TF-IDF performs better than MultinomialNB, but not by much.</p>
<h4>Least Squares</h4>
<p>Finally, let's check how Least Squares performs.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> OneHotEncoder
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> linear_model

<span class="token comment"># Preparing train and test datasets</span>
x_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>encode_documents<span class="token punctuation">(</span>train_df<span class="token punctuation">,</span> group_col<span class="token operator">=</span><span class="token string">"doc_id"</span><span class="token punctuation">,</span> n_words<span class="token operator">=</span>N_WORDS<span class="token punctuation">,</span> enc_type<span class="token operator">=</span><span class="token string">"Multinomial"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
x_test <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>encode_documents<span class="token punctuation">(</span>test_df<span class="token punctuation">,</span> group_col<span class="token operator">=</span><span class="token string">"doc_id"</span><span class="token punctuation">,</span> n_words<span class="token operator">=</span>N_WORDS<span class="token punctuation">,</span> enc_type<span class="token operator">=</span><span class="token string">"Multinomial"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
y_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>get_labels_for_doc_ids<span class="token punctuation">(</span>train_df<span class="token punctuation">)</span><span class="token punctuation">)</span>
y_test <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>get_labels_for_doc_ids<span class="token punctuation">(</span>test_df<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># TF-IDF processing:</span>
tfidf_transformer <span class="token operator">=</span> TfidfTransformer<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># train data processing</span>
x_train <span class="token operator">=</span> tfidf_transformer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># test data processing</span>
x_test <span class="token operator">=</span> tfidf_transformer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Append 1.0 to every doc vector.</span>
one_vec <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
x_train <span class="token operator">=</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> one_vec<span class="token punctuation">)</span><span class="token punctuation">)</span>

one_vec <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>x_test<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
x_test <span class="token operator">=</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> one_vec<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> x_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

<span class="token comment"># OneHotEncode the labels for training</span>
all_labels <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>label_ids_to_labels<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
all_labels<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span>
all_labels <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>all_labels<span class="token punctuation">)</span>

onehot_encoder <span class="token operator">=</span> OneHotEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span>
onehot_labels <span class="token operator">=</span> all_labels<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
onehot_encoder<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>onehot_labels<span class="token punctuation">)</span>
y_train <span class="token operator">=</span> onehot_encoder<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>y_train<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span>
y_train</code></pre></div>
<p>For least squares, we are appending a <code class="language-text">1.0</code> to every document vector. This extra column in the vector acts as the bias parameter for least squares algorithm.</p>
<p>We also need to one hot encode every label so that the least squares algorithm will generate a matrix of size <code class="language-text">mxn</code> where <code class="language-text">m = number of values in document vector</code> and <code class="language-text">n = number of classes</code>. In our case, the parameter matrix has the dimension <code class="language-text">293x20</code>.</p>
<p>We can then find the <code class="language-text">argmax</code> of the prediction vector and find the corresponding class for that max number's position.</p>
<p>If we don't onehotencode our labels, Least Squares will spit out a single number for each document as <code class="language-text">y_train</code> will be a single number for each document.
There will be atleast two problems with this, one about how to deal with fractions, and the other to do with the fact that some classes have small numbers and other large ones.</p>
<p>Suppose two documents belonging to different classes have the label ids <code class="language-text">1</code> and <code class="language-text">20</code> respectively. The parameter that Least Squares learns will be multiplied to every value in the document vector. Thus the parameter when multiplied with two different documents having common words will cause the output to favor one label id over the other.</p>
<p>In order to give every class an equal opportunity regardless of their assigned label number, we use OneHot Encoding technique where we create a vector having all the classes with <code class="language-text">1</code> representing the class the document belongs to and rest of the classes having <code class="language-text">0</code>s.</p>
<p>Now that we understand the processing, let's train our Least Squares classifier:</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment"># learn the parameters</span>
w <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>lstsq<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>

<span class="token comment"># multiply parameters with test data</span>
res <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> w<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

y_pred <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>all_labels<span class="token punctuation">[</span>np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>res<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>res<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
test_accuracy <span class="token operator">=</span> metrics<span class="token punctuation">.</span>accuracy_score<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Least Squares regressor has an accuracy of {:.2f}%"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>test_accuracy<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"against the test set."</span><span class="token punctuation">)</span></code></pre></div>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">Least Squares regressor has an accuracy of 41.51% against the test set.</code></pre></div>
<p>So far, we only checked the accuracy on the test data, so let's take a look at how we'd visualize the model's performance.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> confusion_matrix

cm <span class="token operator">=</span> confusion_matrix<span class="token punctuation">(</span>y_test<span class="token punctuation">,</span> y_pred<span class="token punctuation">)</span>

label_names <span class="token operator">=</span> <span class="token punctuation">[</span>label_ids_to_labels<span class="token punctuation">[</span>all_labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>all_labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

cm_matrix <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>data<span class="token operator">=</span>cm<span class="token punctuation">,</span> columns<span class="token operator">=</span>label_names<span class="token punctuation">,</span> index<span class="token operator">=</span>label_names<span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
sns<span class="token punctuation">.</span>heatmap<span class="token punctuation">(</span>cm_matrix<span class="token punctuation">,</span> annot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> square<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> fmt<span class="token operator">=</span><span class="token string">'d'</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">'YlGnBu'</span><span class="token punctuation">)</span></code></pre></div>
<p><img src="../images/newsgroup-classification-confusion-matrix.jpg" alt="Confusion Matrix for Least Squares"></p>
<p>We can see that documents from the <code class="language-text">sci.electronics</code> class are being incorrectly classified into other classes such as <code class="language-text">comp.graphics</code>, while documents from <code class="language-text">misc.forsale</code> are mostly being correctly identified. This makes sense as there are many classes which have an overlap with <code class="language-text">sci.electronics</code> domain such as <code class="language-text">comp.graphics</code>, <code class="language-text">comp.sys.mac.hardware</code>, and <code class="language-text">sci.med</code> as concepts in electronics are used across many domains and thus the set of words will be common across more classes.</p>
<p>We can also see the same happening with <code class="language-text">alt.atheism</code> and <code class="language-text">soc.religion.christian</code>. The vocabulary both these types of articles use will be similar and thus the model is getting "confused" between the two classes.</p>
<p>Visualizing data can be very powerful and can help us see the biases, outliers, etc in our data and help us decide remedial steps.</p>
<h2>Scope for Improvements</h2>
<p>We have hardly scratched the surface of machine learning/deep learning for text classification.
There are many improvements we could make or experiments we can do to obtain a much better accuracy of classification of text documents.</p>
<p>Some of the improvements/experiments we can do are:</p>
<ol>
<li>Use a larger vocabulary (we used only the top 292 words).</li>
<li>Better pre-processing (removing <code class="language-text">stopwords</code>, performing <code class="language-text">lemmatization</code> etc)</li>
<li>Using other algorithms such as <strong>Gradient boosted trees</strong>, <strong>Support vector machines</strong></li>
<li>Use ordering information / context by using sequence models like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN) and their variations.</li>
<li>Perform hyper-parameter tuning for models.</li>
</ol>
<p>We could also use a much larger dataset to get better results. There's no such thing as too much data.</p>
<h2>Conclusion</h2>
<p>We saw four algorithms for text classification in python and implemented them and checked the results.</p>
<p>In the future, we can expand on this by making improvements as mentioned above.</p>
<p>I hope you liked this article and learned something from it. I'll be posting more articles in this space.</p>
<p>The Jupyter notebook is available here: <a href="https://github.com/shenoy-anurag/machine-learning/blob/main/classification-newsgroup-dataset.ipynb">https://github.com/shenoy-anurag/machine-learning/blob/main/classification-newsgroup-dataset.ipynb</a></p>
<h2>References and Further Reading</h2>
<ol>
<li>Machine Learning: A Probabalistic Approach by Kevin P. Murphy, pg 82-89; pg 103-104; pg 217-220, 2012, MIT Press, ISBN 978-0-262-01802-9 <a href="https://mitpress.mit.edu/books/machine-learning-1">https://mitpress.mit.edu/books/machine-learning-1</a></li>
<li>Naïve Bayes <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes">https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes</a></li>
<li>BernoulliNB Class Documentation <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html">https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html</a></li>
<li>MultinomialNB Class Documentation <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn-naive-bayes-multinomialnb">https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn-naive-bayes-multinomialnb</a></li>
<li>Linear Discriminant Analysis by YANG Xiaozhou <a href="https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b">https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b</a></li>
<li>Least Squares <a href="https://en.wikipedia.org/wiki/Least_squares">https://en.wikipedia.org/wiki/Least_squares</a></li>
<li>TF-IDF <a href="https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089">https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089</a></li>
</ol></div></div></div></main></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/blog/text-classification-on-newsgroup-data/";window.___webpackCompilationHash="081552d3c4ef2a6addac";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-65ceb1e975d78bd881ff.js"],"app":["/app-4025e550f650b75cd59a.js"],"component---src-pages-404-js":["/component---src-pages-404-js-6f7c02045c8a298582d1.js"],"component---src-pages-about-js":["/component---src-pages-about-js-3a791f9dd0d2fec8d001.js"],"component---src-pages-blog-js":["/component---src-pages-blog-js-7a39da1ff7a6a2d1f249.js"],"component---src-pages-index-js":["/component---src-pages-index-js-be5308d0d420bec004e6.js"],"component---src-pages-markdown-remark-frontmatter-slug-js":["/component---src-pages-markdown-remark-frontmatter-slug-js-069fbe32344d374724d4.js"],"component---src-pages-work-js":["/component---src-pages-work-js-036aecaa9ea7f1758396.js"]};/*]]>*/</script><script src="/polyfill-65ceb1e975d78bd881ff.js" nomodule=""></script><script src="/component---src-pages-markdown-remark-frontmatter-slug-js-069fbe32344d374724d4.js" async=""></script><script src="/commons-9d97cb314d58da02d715.js" async=""></script><script src="/app-4025e550f650b75cd59a.js" async=""></script><script src="/framework-0841a708e42dbed148bb.js" async=""></script><script src="/webpack-runtime-b5a1197b4dce08032c07.js" async=""></script></body></html>