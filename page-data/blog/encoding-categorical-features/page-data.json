{
    "componentChunkName": "component---src-pages-markdown-remark-frontmatter-slug-js",
    "path": "/blog/encoding-categorical-features/",
    "result": {"data":{"markdownRemark":{"html":"<h2>Introduction</h2>\n<p>Features can be continuous like temperature or weight or categorical like \"Male\" or \"Female\". The categorical variables are also called Nominal. A nominal category or group is one in which the objects or ideas share characteristics and hence are given a name -- they are nominal.</p>\n<p>There are two common ways to encode categorical features. We can use Ordinal Encoding or we can use One Hot Encoding.</p>\n<h2>Ordinal Encoding <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\">1</a></h2>\n<p>Ordinal encoding involves giving unique integers to each category.</p>\n<p>If we have say \"Male\" or \"Female\", we could assign the category \"Male\" a <code class=\"language-text\">0</code>, and assign \"Female\" a <code class=\"language-text\">1</code>.</p>\n<p>However, ordering in this case doesn't make any sense.</p>\n<p>Let's say we have a model which has the following labels: \"Best\", \"Good\", \"Average\", \"Bad\", \"Worst\". In this example, Ordinal encoding could be useful as we could have integers going from <code class=\"language-text\">0</code> to <code class=\"language-text\">4</code> from \"Worst\" to \"Best\" or the other way around.</p>\n<p>The ordering information could be used by machine learning algorithms in this case to better understand the relationship between these categories.</p>\n<h2>One Hot Encoding</h2>\n<p>One Hot Encoding on the other hand assigns a <code class=\"language-text\">1</code> to a category if it's the correct category for that datapoint or a <code class=\"language-text\">0</code> if not.</p>\n<p>Let's say we have the following categories/classes: \"Laptop\", \"Desktop\", \"Smartphone\", \"Tablet\", \"Smartwatch\".</p>\n<p>If we apply One Hot Encoding, if the class is \"Tablet\", we'd end up with the following array:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>We can see that the 4th position corresponding to \"Tablet\" is given the value <code class=\"language-text\">1</code>, while the rest are <code class=\"language-text\">0</code>s.</p>\n<p>The way I remember this is by remembering the Hot-Cold game.</p>\n<p>In the Hot-Cold game, one person, say A, thinks of a number, say 10, and the other (person B) tries to guess the number.\nIf B guesses a number like 200, A will say \"Cold\".\nIf B guesses a number closer to 10, like say 12, A might say \"Warm\" and as B gets closer and closer, A says warmer and warmer until B guesses correctly which is when A has to say \"Hot\" indicating to B that he guessed correctly.</p>\n<p>One Hot Encoding works in the same way, except we are telling the machine learning algorithm whether their guess (read prediction) is Hot or Not.</p>\n<p>One of the biggest benefits of One Hot Encoding is giving every class an equal opportunity to make an impact.</p>\n<p>This also comes under the topic of Normalisation/Standardisation which is a technique used for input data to allow each feature to have an equal impact on the parameters of the model.</p>\n<h2>Conclusion</h2>\n<p>If you have categorical data, it is a good idea to encode it to improve model performance and accuracy.</p>\n<p>If the categories are ordered, go with Ordinal Encoding.</p>\n<p>If the categories are unordered, go with One Hot Encoding.</p>\n<h2>References &#x26; Further Reading</h2>\n<ol>\n<li>Encodings in Sklearn <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\">https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features</a></li>\n<li>Categorical Variables <a href=\"https://en.wikipedia.org/wiki/Categorical_variable\">https://en.wikipedia.org/wiki/Categorical_variable</a></li>\n<li>One-hot <a href=\"https://en.wikipedia.org/wiki/One-hot\">https://en.wikipedia.org/wiki/One-hot</a></li>\n<li>Normalisation by Zixuan Zhang <a href=\"https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0\">https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0</a></li>\n</ol>","frontmatter":{"date":"February 02, 2022","slug":"/blog/encoding-categorical-features","title":"Encoding Categorical features for Machine Learning"}}},"pageContext":{"id":"6071baa3-78d1-5f10-8d2b-f2d474374db6","frontmatter__slug":"/blog/encoding-categorical-features","__params":{"frontmatter__slug":"blog"}}},
    "staticQueryHashes": ["63159454"]}