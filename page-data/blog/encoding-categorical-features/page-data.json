{
    "componentChunkName": "component---src-pages-markdown-remark-frontmatter-slug-js",
    "path": "/blog/encoding-categorical-features/",
    "result": {"data":{"markdownRemark":{"html":"<h2 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"custom-class before\"><svg width=\"20\" height=\"20\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill-rule=\"evenodd\" clip-rule=\"evenodd\"><path d=\"M14.851 11.923c-.179-.641-.521-1.246-1.025-1.749-1.562-1.562-4.095-1.563-5.657 0l-4.998 4.998c-1.562 1.563-1.563 4.095 0 5.657 1.562 1.563 4.096 1.561 5.656 0l3.842-3.841.333.009c.404 0 .802-.04 1.189-.117l-4.657 4.656c-.975.976-2.255 1.464-3.535 1.464-1.28 0-2.56-.488-3.535-1.464-1.952-1.951-1.952-5.12 0-7.071l4.998-4.998c.975-.976 2.256-1.464 3.536-1.464 1.279 0 2.56.488 3.535 1.464.493.493.861 1.063 1.105 1.672l-.787.784zm-5.703.147c.178.643.521 1.25 1.026 1.756 1.562 1.563 4.096 1.561 5.656 0l4.999-4.998c1.563-1.562 1.563-4.095 0-5.657-1.562-1.562-4.095-1.563-5.657 0l-3.841 3.841-.333-.009c-.404 0-.802.04-1.189.117l4.656-4.656c.975-.976 2.256-1.464 3.536-1.464 1.279 0 2.56.488 3.535 1.464 1.951 1.951 1.951 5.119 0 7.071l-4.999 4.998c-.975.976-2.255 1.464-3.535 1.464-1.28 0-2.56-.488-3.535-1.464-.494-.495-.863-1.067-1.107-1.678l.788-.785z\"/></svg></a>Introduction</h2>\n<p>Features can be continuous like temperature or weight or categorical like \"Male\" or \"Female\". The categorical variables are also called Nominal. A nominal category or group is one in which the objects or ideas share characteristics and hence are given a name -- they are nominal.</p>\n<p>There are two common ways to encode categorical features. We can use Ordinal Encoding or we can use One Hot Encoding.</p>\n<h2 id=\"ordinal-encoding-1\" style=\"position:relative;\"><a href=\"#ordinal-encoding-1\" aria-label=\"ordinal encoding 1 permalink\" class=\"custom-class before\"><svg width=\"20\" height=\"20\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill-rule=\"evenodd\" clip-rule=\"evenodd\"><path d=\"M14.851 11.923c-.179-.641-.521-1.246-1.025-1.749-1.562-1.562-4.095-1.563-5.657 0l-4.998 4.998c-1.562 1.563-1.563 4.095 0 5.657 1.562 1.563 4.096 1.561 5.656 0l3.842-3.841.333.009c.404 0 .802-.04 1.189-.117l-4.657 4.656c-.975.976-2.255 1.464-3.535 1.464-1.28 0-2.56-.488-3.535-1.464-1.952-1.951-1.952-5.12 0-7.071l4.998-4.998c.975-.976 2.256-1.464 3.536-1.464 1.279 0 2.56.488 3.535 1.464.493.493.861 1.063 1.105 1.672l-.787.784zm-5.703.147c.178.643.521 1.25 1.026 1.756 1.562 1.563 4.096 1.561 5.656 0l4.999-4.998c1.563-1.562 1.563-4.095 0-5.657-1.562-1.562-4.095-1.563-5.657 0l-3.841 3.841-.333-.009c-.404 0-.802.04-1.189.117l4.656-4.656c.975-.976 2.256-1.464 3.536-1.464 1.279 0 2.56.488 3.535 1.464 1.951 1.951 1.951 5.119 0 7.071l-4.999 4.998c-.975.976-2.255 1.464-3.535 1.464-1.28 0-2.56-.488-3.535-1.464-.494-.495-.863-1.067-1.107-1.678l.788-.785z\"/></svg></a>Ordinal Encoding <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\">1</a></h2>\n<p>Ordinal encoding involves giving unique integers to each category.</p>\n<p>If we have say \"Male\" or \"Female\", we could assign the category \"Male\" a <code class=\"language-text\">0</code>, and assign \"Female\" a <code class=\"language-text\">1</code>.</p>\n<p>However, ordering in this case doesn't make any sense.</p>\n<p>Let's say we have a model which has the following labels: \"Best\", \"Good\", \"Average\", \"Bad\", \"Worst\". In this example, Ordinal encoding could be useful as we could have integers going from <code class=\"language-text\">0</code> to <code class=\"language-text\">4</code> from \"Worst\" to \"Best\" or the other way around.</p>\n<p>The ordering information could be used by machine learning algorithms in this case to better understand the relationship between these categories.</p>\n<h2 id=\"one-hot-encoding\" style=\"position:relative;\"><a href=\"#one-hot-encoding\" aria-label=\"one hot encoding permalink\" class=\"custom-class before\"><svg width=\"20\" height=\"20\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill-rule=\"evenodd\" clip-rule=\"evenodd\"><path d=\"M14.851 11.923c-.179-.641-.521-1.246-1.025-1.749-1.562-1.562-4.095-1.563-5.657 0l-4.998 4.998c-1.562 1.563-1.563 4.095 0 5.657 1.562 1.563 4.096 1.561 5.656 0l3.842-3.841.333.009c.404 0 .802-.04 1.189-.117l-4.657 4.656c-.975.976-2.255 1.464-3.535 1.464-1.28 0-2.56-.488-3.535-1.464-1.952-1.951-1.952-5.12 0-7.071l4.998-4.998c.975-.976 2.256-1.464 3.536-1.464 1.279 0 2.56.488 3.535 1.464.493.493.861 1.063 1.105 1.672l-.787.784zm-5.703.147c.178.643.521 1.25 1.026 1.756 1.562 1.563 4.096 1.561 5.656 0l4.999-4.998c1.563-1.562 1.563-4.095 0-5.657-1.562-1.562-4.095-1.563-5.657 0l-3.841 3.841-.333-.009c-.404 0-.802.04-1.189.117l4.656-4.656c.975-.976 2.256-1.464 3.536-1.464 1.279 0 2.56.488 3.535 1.464 1.951 1.951 1.951 5.119 0 7.071l-4.999 4.998c-.975.976-2.255 1.464-3.535 1.464-1.28 0-2.56-.488-3.535-1.464-.494-.495-.863-1.067-1.107-1.678l.788-.785z\"/></svg></a>One Hot Encoding</h2>\n<p>One Hot Encoding on the other hand assigns a <code class=\"language-text\">1</code> to a category if it's the correct category for that datapoint or a <code class=\"language-text\">0</code> if not.</p>\n<p>Let's say we have the following categories/classes: \"Laptop\", \"Desktop\", \"Smartphone\", \"Tablet\", \"Smartwatch\".</p>\n<p>If we apply One Hot Encoding, if the class is \"Tablet\", we'd end up with the following array:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>We can see that the 4th position corresponding to \"Tablet\" is given the value <code class=\"language-text\">1</code>, while the rest are <code class=\"language-text\">0</code>s.</p>\n<p>The way I remember this is by remembering the Hot-Cold game.</p>\n<p>In the Hot-Cold game, one person, say A, thinks of a number, say 10, and the other (person B) tries to guess the number.\nIf B guesses a number like 200, A will say \"Cold\".\nIf B guesses a number closer to 10, like say 12, A might say \"Warm\" and as B gets closer and closer, A says warmer and warmer until B guesses correctly which is when A has to say \"Hot\" indicating to B that he guessed correctly.</p>\n<p>One Hot Encoding works in the same way, except we are telling the machine learning algorithm whether their guess (read prediction) is Hot or Not.</p>\n<p>One of the biggest benefits of One Hot Encoding is giving every class an equal opportunity to make an impact.</p>\n<p>This also comes under the topic of Normalisation/Standardisation which is a technique used for input data to allow each feature to have an equal impact on the parameters of the model.</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"custom-class before\"><svg width=\"20\" height=\"20\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill-rule=\"evenodd\" clip-rule=\"evenodd\"><path d=\"M14.851 11.923c-.179-.641-.521-1.246-1.025-1.749-1.562-1.562-4.095-1.563-5.657 0l-4.998 4.998c-1.562 1.563-1.563 4.095 0 5.657 1.562 1.563 4.096 1.561 5.656 0l3.842-3.841.333.009c.404 0 .802-.04 1.189-.117l-4.657 4.656c-.975.976-2.255 1.464-3.535 1.464-1.28 0-2.56-.488-3.535-1.464-1.952-1.951-1.952-5.12 0-7.071l4.998-4.998c.975-.976 2.256-1.464 3.536-1.464 1.279 0 2.56.488 3.535 1.464.493.493.861 1.063 1.105 1.672l-.787.784zm-5.703.147c.178.643.521 1.25 1.026 1.756 1.562 1.563 4.096 1.561 5.656 0l4.999-4.998c1.563-1.562 1.563-4.095 0-5.657-1.562-1.562-4.095-1.563-5.657 0l-3.841 3.841-.333-.009c-.404 0-.802.04-1.189.117l4.656-4.656c.975-.976 2.256-1.464 3.536-1.464 1.279 0 2.56.488 3.535 1.464 1.951 1.951 1.951 5.119 0 7.071l-4.999 4.998c-.975.976-2.255 1.464-3.535 1.464-1.28 0-2.56-.488-3.535-1.464-.494-.495-.863-1.067-1.107-1.678l.788-.785z\"/></svg></a>Conclusion</h2>\n<p>If you have categorical data, it is a good idea to encode it to improve model performance and accuracy.</p>\n<p>If the categories are ordered, go with Ordinal Encoding.</p>\n<p>If the categories are unordered, go with One Hot Encoding.</p>\n<h2 id=\"references--further-reading\" style=\"position:relative;\"><a href=\"#references--further-reading\" aria-label=\"references  further reading permalink\" class=\"custom-class before\"><svg width=\"20\" height=\"20\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill-rule=\"evenodd\" clip-rule=\"evenodd\"><path d=\"M14.851 11.923c-.179-.641-.521-1.246-1.025-1.749-1.562-1.562-4.095-1.563-5.657 0l-4.998 4.998c-1.562 1.563-1.563 4.095 0 5.657 1.562 1.563 4.096 1.561 5.656 0l3.842-3.841.333.009c.404 0 .802-.04 1.189-.117l-4.657 4.656c-.975.976-2.255 1.464-3.535 1.464-1.28 0-2.56-.488-3.535-1.464-1.952-1.951-1.952-5.12 0-7.071l4.998-4.998c.975-.976 2.256-1.464 3.536-1.464 1.279 0 2.56.488 3.535 1.464.493.493.861 1.063 1.105 1.672l-.787.784zm-5.703.147c.178.643.521 1.25 1.026 1.756 1.562 1.563 4.096 1.561 5.656 0l4.999-4.998c1.563-1.562 1.563-4.095 0-5.657-1.562-1.562-4.095-1.563-5.657 0l-3.841 3.841-.333-.009c-.404 0-.802.04-1.189.117l4.656-4.656c.975-.976 2.256-1.464 3.536-1.464 1.279 0 2.56.488 3.535 1.464 1.951 1.951 1.951 5.119 0 7.071l-4.999 4.998c-.975.976-2.255 1.464-3.535 1.464-1.28 0-2.56-.488-3.535-1.464-.494-.495-.863-1.067-1.107-1.678l.788-.785z\"/></svg></a>References &#x26; Further Reading</h2>\n<ol>\n<li>Encodings in Sklearn <a href=\"https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\">https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features</a></li>\n<li>Categorical Variables <a href=\"https://en.wikipedia.org/wiki/Categorical_variable\">https://en.wikipedia.org/wiki/Categorical_variable</a></li>\n<li>One-hot <a href=\"https://en.wikipedia.org/wiki/One-hot\">https://en.wikipedia.org/wiki/One-hot</a></li>\n<li>Normalisation by Zixuan Zhang <a href=\"https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0\">https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0</a></li>\n</ol>","frontmatter":{"date":"February 02, 2022","slug":"/blog/encoding-categorical-features","title":"Encoding Categorical features for Machine Learning"}}},"pageContext":{"id":"26375fdb-bc9c-5a6f-8e28-c8f10375d674","frontmatter__slug":"/blog/encoding-categorical-features","__params":{"frontmatter__slug":"blog"}}},
    "staticQueryHashes": ["63159454"]}